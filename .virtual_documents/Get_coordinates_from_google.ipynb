import time
import json
import urllib.parse
import re
import time
import requests

import pandas as pd

from selenium import webdriver
from tqdm import tqdm
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By
from selenium.common.exceptions import TimeoutException


driver = webdriver.Chrome()  # Optional argument, if not specified will search path.



df = pd.read_csv("data/Processing/09_11_2023_xa_ds_latlong.csv")
df.groupby("level")["Pop"].sum()


to_be_done = df[df["lat"].isna()].apply(
    lambda x: f"{x['ADM3']}, {x['ADM2']}, {x['ADM1']}",
    axis=1
).values
to_be_done[0:3]


def replace_prefix(x):
  if isinstance(x, str):
    x = x.replace("Xã ","")
    x = x.replace("Thị trấn ","")
    x = x.replace("Phường ","")
    x = x.replace("Thị Trấn ","")
    x = x.replace("Huyện ","")
    x = x.replace("Thành phố ","")
    x = x.replace("Thị xã ","")
    x = x.replace("Tỉnh ","")
  return x
def replace_prefixADM3(x):
  if isinstance(x, str):
    x = x.replace("Xã ","")
    x = x.replace("Thị trấn ","")
    x = x.replace("Phường ","")
  return x


results_google = [None]*len(to_be_done)




# remember to check the browser for any agreement
# repeat until all the phuong xa have lat lon
while any(x is None for x in results_google):
    for i in tqdm(range(len(to_be_done))):
        if results_google[i] is not None:
            continue
        driver.get(
            f'https://www.google.com/maps/search/{urllib.parse.quote(replace_prefix(to_be_done[i]))}'
        )
        time.sleep(2)
        while driver.current_url == f'https://www.google.com/maps/search/{urllib.parse.quote(replace_prefix(to_be_done[i]))}':
            time.sleep(1)
        if re.search(r"(?P<lat>[1-2]?[0-9]\.[0-9]+),(?P<long>[0-9]{2,3}\.[0-9]+)",driver.current_url):
            results_google[i] = re.search(r"(?P<lat>[1-2]?[0-9]\.[0-9]+),(?P<long>[0-9]{2,3}\.[0-9]+)",driver.current_url).groups()




df_google = pd.DataFrame(to_be_done, columns=["Location"])
df_google_lat_long = pd.DataFrame(
    [(None, None) if x is None
     else (x[0],x[1]) for x in results_google], columns=["lat","lon"])
df_google_lat_long = pd.concat([df_google,df_google_lat_long], axis=1)
df_google_lat_long


df


# augment working data with data from google
df[["lat","lon"]] = df.apply(
    lambda x: x[["lat","lon"]] if not pd.isna(x.lat) 
    else df_google_lat_long[df_google_lat_long['Location']==f"{x['ADM3']}, {x['ADM2']}, {x['ADM1']}"][["lat","lon"]].values[0],
    axis=1
)















df.to_csv("from_google_lat_long_4.csv")


df


df = pd.read_csv("for_google_2.csv")
to_be_done = df.apply(
    lambda x: f"{x['ADM3']}, {x['ADM2']}, {x['ADM1']}",
    axis=1
).values



df = pd.read_csv("09_11_2023_xa_ds_latlong.csv")
df.groupby("level")["Pop"].sum()


import pandas as pd

df_google_done = pd.read_csv("./from_google_lat_long.csv")
df_google_done_2 = pd.read_csv("./from_google_lat_long_2.csv")
df_google_done_3 = pd.read_csv("./from_google_lat_long_3.csv")
df_google_done_4 = pd.read_csv("./from_google_lat_long_4.csv")


df_google_done['0'] = df_google_done['0'].apply(
    lambda x: replace_prefixADM3(x)
)
df_google = pd.concat([df_google_done,df_google_done_2, df_google_done_3, df_google_done_4], ignore_index=True)
df_google = df_google[~df_google.lat.isna()]
df_google


df[df["lat"].isna()].apply(lambda x: df_google[df_google['0']==f"{x['ADM3']}, {x['ADM2']}, {x['ADM1']}"][["lat","long"]].values, axis=1)


to_be_done = df[df["lat"].isna() & df.apply(lambda x: f"{x['ADM3']}, {x['ADM2']}, {x['ADM1']}" not in df_google['0'].values, axis=1)].apply(
    lambda x: f"{x['ADM3']}, {x['ADM2']}, {x['ADM1']}",
    axis=1
).values


tmp = df[df["lat"].isna()].apply(lambda x: df_google[df_google['0']==f"{x['ADM3']}, {x['ADM2']}, {x['ADM1']}"][["lat","long"]].values[0], axis=1)


df[["lat","lon"]] = df.apply(
    lambda x: x[["lat","lon"]] if not pd.isna(x.lat) else df_google[df_google['0']==f"{x['ADM3']}, {x['ADM2']}, {x['ADM1']}"][["lat","long"]].values[0],
    axis=1
)


import pandas as pd
df = pd.read_csv("10_11_2023_danso_toado.csv")




session = requests.Session()

def get_signals(lat, long):
  n = 1
  while n<300:
    try:
      print(f"https://opennetwork.viettel.vn/user-ws/troubleshoot/tile/stats/l4/{lat}/{long}")
      response = session.get(
          f"https://opennetwork.viettel.vn/user-ws/troubleshoot/tile/stats/l4/{lat}/{long}",
          timeout=10
      )
      #time.sleep(1)
      print(response.text)
      return response.text
    except KeyboardInterrupt:
        raise
    except Exception as e:
      print(e)
      n = n * 1.5
      time.sleep(n)
  raise
signals = [None] * len(df_x)
for i,(lat, lon) in enumerate(tqdm(df_x[["lat", "lon"]].values)):
    if signals[i] is None:
        signals[i] = get_signals(lat, lon)
    
    


df_x


pd.DataFrame([ast.literal_eval(signal) for signal in signals])



import json
json.dump(signals, open("all_signals.json", "w"))


df


import ast

df_ds_signals = pd.concat([df, df_signals],  axis=1)
df_ds_signals[df_ds_signals["label"]=="Không có thông tin"]
to_be_done = df_ds_signals[df_ds_signals["label"]=="Không có thông tin"].apply(
    lambda x: f"{x['ADM3']}, {x['ADM2']}, {x['ADM1']}",
    axis=1
).values
len(to_be_done)





#df_ds_signals[df_ds_signals["label"]=="Không có thông tin"][["lat","lon"]] = results_google
df_x = df_ds_signals.copy()
df_x.loc[df_ds_signals["label"]=="Không có thông tin", "x"] = pd.DataFrame([ast.literal_eval(signal) for signal in signals])[["x", "y", "level", "label", "colorCode"]].values[:,0]
df_x.loc[df_ds_signals["label"]=="Không có thông tin", "y"] = pd.DataFrame([ast.literal_eval(signal) for signal in signals])[["x", "y", "level", "label", "colorCode"]].values[:,1]
#df_x.loc[df_ds_signals["label"]=="Không có thông tin", "level"] = pd.DataFrame([ast.literal_eval(signal) for signal in signals])[["x", "y", "level", "label", "colorCode"]].values[:,2]
df_x.loc[df_ds_signals["label"]=="Không có thông tin", "label"] = pd.DataFrame([ast.literal_eval(signal) for signal in signals])[["x", "y", "level", "label", "colorCode"]].values[:,3]
df_x.loc[df_ds_signals["label"]=="Không có thông tin", "colorCode"] = pd.DataFrame([ast.literal_eval(signal) for signal in signals])[["x", "y", "level", "label", "colorCode"]].values[:,4]



df_x[df_ds_signals["label"]=="Không có thông tin"]


df_x.drop_duplicates(subset=["ADM1", "ADM2", "ADM3", "Pop"]).groupby("label").Pop.sum()


df_x.to_csv("ds_toado_signal_final_10_11_2023.csv", index=None)


df_x[df_x["label"] == "Không có thông tin"]


import math

def haversine_df(df, lat2, lon2):
    return df.apply(
        lambda x: haversine(x["lat"], x["long"], lat2, lon2),
        axis=1
    )

def haversine(lat1, lon1, lat2, lon2):
    # Radius of the Earth in km
    R = 6371.0

    # Convert latitude and longitude from degrees to radians
    lat1_rad = math.radians(lat1)
    lon1_rad = math.radians(lon1)
    lat2_rad = math.radians(lat2)
    lon2_rad = math.radians(lon2)

    # Difference in coordinates
    dlat = lat2_rad - lat1_rad
    dlon = lon2_rad - lon1_rad

    # Haversine formula
    a = math.sin(dlat / 2)**2 + math.cos(lat1_rad) * math.cos(lat2_rad) * math.sin(dlon / 2)**2
    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))
    distance = R * c

    # Convert distance from km to meters
    return distance * 1000

df_finally["most_similar_d"] = df_finally[["lat","lon"]].apply(
    lambda x: haversine_df(
        df_signals_lat_long,
        x["lat"], x["lon"]
    ).min(),
    axis=1
)

